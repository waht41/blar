from datetime import datetime
import argparse

import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import BaseCallback
import os
import time

from src.callbacks import VisualizationCallback


def main():
    """ä¸»å‡½æ•°ï¼šè®­ç»ƒLunarLander PPOæ¨¡å‹"""
    # --- 1. è§£æå‘½ä»¤è¡Œå‚æ•° ---
    parser = argparse.ArgumentParser(description='è®­ç»ƒLunarLander PPOæ¨¡å‹')
    parser.add_argument('--resume', type=str, help='ä»æŒ‡å®šè·¯å¾„æ¢å¤è®­ç»ƒæ¨¡å‹')
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸš€ LunarLander PPO è®­ç»ƒç¨‹åºå¯åŠ¨")
    print("=" * 60)
    
    # --- 2. åˆ›å»ºç¯å¢ƒå’Œæ—¥å¿—ç›®å½• ---
    print("ğŸ“¦ æ­£åœ¨åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    # è®­ç»ƒç¯å¢ƒï¼Œä½¿ç”¨å¹¶è¡ŒåŒ–åŠ é€Ÿ
    train_env = make_vec_env("LunarLander-v3", n_envs=16)
    print(f"âœ… è®­ç»ƒç¯å¢ƒåˆ›å»ºå®Œæˆï¼Œä½¿ç”¨ {16} ä¸ªå¹¶è¡Œç¯å¢ƒ")
    
    # å•ç‹¬åˆ›å»ºä¸€ä¸ªç”¨äºå¯è§†åŒ–çš„ç¯å¢ƒ
    eval_env = gym.make("LunarLander-v3", render_mode="human")
    print("âœ… å¯è§†åŒ–ç¯å¢ƒåˆ›å»ºå®Œæˆ")
    
    # è®¾ç½®æ—¥å¿—ç›®å½•
    if args.resume:
        # å¦‚æœæ˜¯ä»ç°æœ‰æ¨¡å‹æ¢å¤ï¼Œä½¿ç”¨åŸæ¨¡å‹çš„æ—¥å¿—ç›®å½•
        model_dir = os.path.dirname(args.resume)
        log_dir = model_dir
        print(f"ğŸ”„ ä»ç°æœ‰æ¨¡å‹æ¢å¤è®­ç»ƒ: {args.resume}")
        print(f"ğŸ“ ä½¿ç”¨ç°æœ‰æ—¥å¿—ç›®å½•: {log_dir}")
    else:
        # åˆ›å»ºæ–°çš„æ—¥å¿—ç›®å½•
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        run_id = f"{timestamp}_ppo_lunarlander_logs"
        log_dir = f"./logs/{run_id}/"
        os.makedirs(log_dir, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºæ–°æ—¥å¿—ç›®å½•: {log_dir}")
    
    model_path = os.path.join(log_dir, "lunar_lander_model.zip")

    # --- 3. å®ä¾‹åŒ–å›è°ƒå‡½æ•° ---
    print("ğŸ¯ è®¾ç½®å¯è§†åŒ–å›è°ƒå‡½æ•°...")
    # è®¾ç½®æ¯ 20,000 æ­¥å¯è§†åŒ–ä¸€æ¬¡
    vis_callback = VisualizationCallback(eval_env, eval_freq=20000)
    print("âœ… å¯è§†åŒ–å›è°ƒå‡½æ•°è®¾ç½®å®Œæˆï¼Œæ¯20000æ­¥å±•ç¤ºä¸€æ¬¡æ•ˆæœ")

    # --- 4. åˆ›å»ºæˆ–åŠ è½½ PPO æ¨¡å‹ ---
    if args.resume:
        print("ğŸ”„ æ­£åœ¨åŠ è½½ç°æœ‰æ¨¡å‹...")
        if not os.path.exists(args.resume):
            print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.resume}")
            return
        
        model = PPO.load(args.resume, env=train_env, reset_num_timesteps=False)
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {args.resume}")
        print("ğŸ“Š å°†ç»§ç»­ä¹‹å‰çš„è®­ç»ƒè¿›åº¦")
    else:
        print("ğŸ†• æ­£åœ¨åˆ›å»ºæ–°çš„PPOæ¨¡å‹...")
        model = PPO(
            "MlpPolicy",
            train_env,
            verbose=1,
            tensorboard_log=log_dir,
            device='auto'
        )
        print("âœ… æ–°PPOæ¨¡å‹åˆ›å»ºå®Œæˆ")

    print("\n" + "=" * 60)
    print("ğŸƒâ€â™‚ï¸ å¼€å§‹è®­ç»ƒ...")
    print("=" * 60)
    
    # åœ¨ learn() æ–¹æ³•ä¸­ä¼ å…¥ callback
    model.learn(
        total_timesteps=200_000,
        tb_log_name="PPO_with_vis",
        callback=vis_callback
    )
    
    print("\n" + "=" * 60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)

    # --- 5. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.save(model_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    print(f"ğŸ“Š è¦æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼Œè¯·åœ¨ç»ˆç«¯è¿è¡Œ: tensorboard --logdir ./logs")
    
    # --- 6. æ¸…ç†ç¯å¢ƒ ---
    print("ğŸ§¹ æ­£åœ¨æ¸…ç†ç¯å¢ƒ...")
    train_env.close()
    eval_env.close()
    print("âœ… ç¯å¢ƒæ¸…ç†å®Œæˆ")
    print("ğŸ‘‹ ç¨‹åºç»“æŸ")


if __name__ == "__main__":
    main()

