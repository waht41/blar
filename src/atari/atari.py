import gymnasium as gym
import ale_py


from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack
import os

from src.callbacks import VisualizationCallback
from src.utils.training_utils import setup_training_args_and_logs, print_training_header, print_training_footer


def main():
    """ä¸»å‡½æ•°ï¼šè®­ç»ƒAtari Breakout PPOæ¨¡å‹"""
    # --- 1. è§£æå‘½ä»¤è¡Œå‚æ•°å’Œè®¾ç½®æ—¥å¿—ç›®å½• ---
    args, log_dir, model_path = setup_training_args_and_logs(
        game_name="AtariBreakout",
        model_name="atari_breakout_model"
    )
    gym.register_envs(ale_py)
    
    print_training_header("Atari Breakout")
    
    # --- 2. åˆ›å»ºç¯å¢ƒ ---
    print("ğŸ“¦ æ­£åœ¨åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    # è®­ç»ƒç¯å¢ƒï¼Œä½¿ç”¨å¹¶è¡ŒåŒ–åŠ é€Ÿ
    # make_atari_env ä¼šè‡ªåŠ¨å¤„ç†å¤§éƒ¨åˆ†é¢„å¤„ç†å·¥ä½œ
    train_env = make_atari_env('ALE/Breakout-v5', n_envs=4, seed=0)
    # VecFrameStack å°†è¿ç»­çš„4å¸§å›¾åƒå †å èµ·æ¥ï¼Œè®©æ™ºèƒ½ä½“èƒ½æ„ŸçŸ¥åˆ°è¿åŠ¨æ–¹å‘
    train_env = VecFrameStack(train_env, n_stack=4)
    print(f"âœ… è®­ç»ƒç¯å¢ƒåˆ›å»ºå®Œæˆï¼Œä½¿ç”¨ {4} ä¸ªå¹¶è¡Œç¯å¢ƒ")
    
    # å•ç‹¬åˆ›å»ºä¸€ä¸ªç”¨äºå¯è§†åŒ–çš„ç¯å¢ƒ
    eval_env = make_atari_env('ALE/Breakout-v5', n_envs=1, seed=1)  # ä½¿ç”¨ä¸åŒçš„seedé¿å…å’Œè®­ç»ƒç¯å¢ƒå®Œå…¨ä¸€æ ·
    eval_env = VecFrameStack(eval_env, n_stack=4)
    # eval_env = gym.make('ALE/Breakout-v5', render_mode='human')
    # eval_env = VecFrameStack(eval_env, n_stack=4)
    print("âœ… å¯è§†åŒ–ç¯å¢ƒåˆ›å»ºå®Œæˆ")

    # --- 3. å®ä¾‹åŒ–å›è°ƒå‡½æ•° ---
    print("ğŸ¯ è®¾ç½®å¯è§†åŒ–å›è°ƒå‡½æ•°...")
    # è®¾ç½®æ¯ 50,000 æ­¥å¯è§†åŒ–ä¸€æ¬¡ï¼ˆAtariæ¸¸æˆè®­ç»ƒæ—¶é—´è¾ƒé•¿ï¼‰
    vis_callback = VisualizationCallback(eval_env, eval_freq=50000)
    print("âœ… å¯è§†åŒ–å›è°ƒå‡½æ•°è®¾ç½®å®Œæˆï¼Œæ¯50000æ­¥å±•ç¤ºä¸€æ¬¡æ•ˆæœ")

    # --- 4. åˆ›å»ºæˆ–åŠ è½½ PPO æ¨¡å‹ ---
    if args.resume:
        print("ğŸ”„ æ­£åœ¨åŠ è½½ç°æœ‰æ¨¡å‹...")
        if not os.path.exists(args.resume):
            print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.resume}")
            return
        
        model = PPO.load(args.resume, env=train_env, reset_num_timesteps=False)
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {args.resume}")
        print("ğŸ“Š å°†ç»§ç»­ä¹‹å‰çš„è®­ç»ƒè¿›åº¦")
    else:
        print("ğŸ†• æ­£åœ¨åˆ›å»ºæ–°çš„PPOæ¨¡å‹...")
        # ä½¿ç”¨ CnnPolicy åˆ›å»º PPO æ¨¡å‹ï¼Œæ³¨æ„è¿™é‡Œçš„ç­–ç•¥å˜æˆäº† "CnnPolicy"
        model = PPO(
            "CnnPolicy",
            train_env,
            verbose=1,
            tensorboard_log=log_dir,
            device='auto'
        )
        print("âœ… æ–°PPOæ¨¡å‹åˆ›å»ºå®Œæˆ")

    print("\n" + "=" * 60)
    print("ğŸƒâ€â™‚ï¸ å¼€å§‹è®­ç»ƒ...")
    print("=" * 60)
    
    # åœ¨ learn() æ–¹æ³•ä¸­ä¼ å…¥ callback
    # Atari æ¸¸æˆé€šå¸¸éœ€è¦æ›´é•¿çš„è®­ç»ƒæ—¶é—´ï¼Œä¾‹å¦‚å‡ ç™¾ä¸‡æ­¥
    model.learn(
        total_timesteps=1_000_000,
        tb_log_name="PPO_with_vis",
        callback=vis_callback
    )
    
    print_training_footer(log_dir)

    # --- 5. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.save(model_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")
    
    # --- 6. æ¸…ç†ç¯å¢ƒ ---
    print("ğŸ§¹ æ­£åœ¨æ¸…ç†ç¯å¢ƒ...")
    train_env.close()
    eval_env.close()
    print("âœ… ç¯å¢ƒæ¸…ç†å®Œæˆ")
    print("ğŸ‘‹ ç¨‹åºç»“æŸ")


if __name__ == "__main__":
    main()