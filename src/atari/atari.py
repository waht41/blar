import ale_py # ale_py éœ€è¦è¢« import ä¸€æ¬¡ï¼Œè®© gym èƒ½å¤Ÿå‘ç° Atari ç¯å¢ƒ
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack
from stable_baselines3.common.callbacks import CallbackList
import os
from typing import Callable  # å¯¼å…¥ Callable ç”¨äºå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦

from src.callbacks import VisualizationCallback, PerformanceCallbackWithTqdm
from src.utils.training_utils import setup_training_args_and_logs, print_training_header, print_training_footer
ale_py # å¼•å…¥ç¯å¢ƒï¼Œç”¨æ¥è®©importä¸è¢«ideè‡ªåŠ¨åˆ é™¤

def linear_schedule(initial_value: float) -> Callable[[float], float]:
    """
    åˆ›å»ºä¸€ä¸ªçº¿æ€§å­¦ä¹ ç‡è¡°å‡çš„è°ƒåº¦å™¨ã€‚
    :param initial_value: åˆå§‹å­¦ä¹ ç‡
    :return: ä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥è¿›åº¦(0-1)ï¼Œè¾“å‡ºå½“å‰å­¦ä¹ ç‡
    """

    def func(progress_remaining: float) -> float:
        """
        éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œå­¦ä¹ ç‡ä» initial_value çº¿æ€§é™ä½åˆ° 0.
        progress_remaining ä» 1 çº¿æ€§é™ä½åˆ° 0.
        """
        return progress_remaining * initial_value

    return func


def main():
    """ä¸»å‡½æ•°ï¼šè®­ç»ƒAtari Breakout PPOæ¨¡å‹"""
    # --- 1. è§£æå‘½ä»¤è¡Œå‚æ•°å’Œè®¾ç½®æ—¥å¿—ç›®å½• ---
    args, log_dir, model_path = setup_training_args_and_logs(
        game_name="AtariBreakout",
        model_name="atari_breakout_model"
    )

    # gym.register_envs(ale_py) # è¿™è¡Œæ˜¯ä¸éœ€è¦çš„ï¼Œåªè¦ ale_py è¢«å¯¼å…¥ï¼Œgym å°±ä¼šè‡ªåŠ¨æ³¨å†Œç¯å¢ƒ

    print_training_header("Atari Breakout")

    # --- 2. åˆ›å»ºç¯å¢ƒ ---
    print("ğŸ“¦ æ­£åœ¨åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    # ä½¿ç”¨ make_atari_env ä¼šè‡ªåŠ¨åº”ç”¨ä¸€ç³»åˆ—å…³é”®çš„ Wrapperï¼Œä¾‹å¦‚å¸§è·³è¿‡(Frame Skipping)
    n_envs = 16
    train_env = make_atari_env('ALE/Breakout-v5', n_envs=n_envs, seed=0)
    # VecFrameStack å°†è¿ç»­çš„4å¸§å›¾åƒå †å èµ·æ¥ï¼Œè®©æ™ºèƒ½ä½“èƒ½æ„ŸçŸ¥åˆ°è¿åŠ¨æ–¹å‘
    train_env = VecFrameStack(train_env, n_stack=4)
    print(f"âœ… è®­ç»ƒç¯å¢ƒåˆ›å»ºå®Œæˆï¼Œä½¿ç”¨ {n_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ")

    eval_env = make_atari_env('ALE/Breakout-v5', n_envs=1, seed=1)
    eval_env = VecFrameStack(eval_env, n_stack=4)
    print("âœ… å¯è§†åŒ–ç¯å¢ƒåˆ›å»ºå®Œæˆ")

    # --- 3. å®ä¾‹åŒ–å›è°ƒå‡½æ•° ---
    print("ğŸ¯ è®¾ç½®å›è°ƒå‡½æ•°...")
    # ç”±äºæ€»æ­¥æ•°å¢åŠ ï¼Œå¯ä»¥é€‚å½“å¢åŠ å›è°ƒé¢‘ç‡
    vis_callback = VisualizationCallback(eval_env, eval_freq=50000)  # eval_freq æ˜¯åŸºäºå•ä¸ªç¯å¢ƒçš„æ­¥æ•°
    performance_callback = PerformanceCallbackWithTqdm(verbose=1)
    
    # ç»„åˆå¤šä¸ªå›è°ƒå‡½æ•°
    callbacks = CallbackList([vis_callback, performance_callback])
    print("âœ… å›è°ƒå‡½æ•°è®¾ç½®å®Œæˆ (å¯è§†åŒ– + æ€§èƒ½ç›‘æ§)")

    # --- 4. å®šä¹‰è¶…å‚æ•°å¹¶åˆ›å»ºæˆ–åŠ è½½ PPO æ¨¡å‹ ---

    # å…³é”®ä¼˜åŒ–ï¼šä¸ºAtariè®¾ç½®ä¸€å¥—æ›´ç¨³å®šçš„è¶…å‚æ•°
    # å‚è€ƒäº† Stable Baselines3 Zoo å’Œç›¸å…³è®ºæ–‡çš„æ¨èå€¼
    learning_rate = 2.5e-4
    ppo_params = {
        'n_steps': 2048,  # å¢åŠ æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ ·æœ¬æ•°ï¼Œä»¥è·å¾—æ›´ç¨³å®šçš„æ¢¯åº¦ä¼°è®¡
        'batch_size': 512,  # å¢åŠ  mini-batch çš„å¤§å°
        'n_epochs': 4,  # å‡å°‘ epoch æ•°é‡ï¼Œé˜²æ­¢åœ¨å½“å‰æ•°æ®ä¸Šè¿‡æ‹Ÿåˆ
        'gamma': 0.99,  # æŠ˜æ‰£å› å­
        'gae_lambda': 0.95,  # GAE-Lambda å‚æ•°
        'clip_range': 0.1,  # å‡å° clip_rangeï¼Œé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œæå‡ç¨³å®šæ€§
        'ent_coef': 0.01,  # ç†µç³»æ•°ï¼Œé¼“åŠ±æ¢ç´¢
        'vf_coef': 0.5,  # ä»·å€¼å‡½æ•°ç³»æ•°
        'learning_rate': linear_schedule(learning_rate),  # ä½¿ç”¨çº¿æ€§è¡°å‡çš„å­¦ä¹ ç‡ï¼Œä» 2.5e-4 é™åˆ° 0
    }

    if args.resume:
        print("ğŸ”„ æ­£åœ¨åŠ è½½ç°æœ‰æ¨¡å‹...")
        if not os.path.exists(args.resume):
            print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.resume}")
            return

        model = PPO.load(
            args.resume,
            env=train_env,
            reset_num_timesteps=False,
            # åŠ è½½æ¨¡å‹æ—¶ä¹Ÿæœ€å¥½ä¼ å…¥è‡ªå®šä¹‰å‚æ•°ï¼Œä»¥é˜²é»˜è®¤å‚æ•°è¦†ç›–
            custom_objects={"learning_rate": ppo_params['learning_rate']}
        )
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {args.resume}")
        print("ğŸ“Š å°†ç»§ç»­ä¹‹å‰çš„è®­ç»ƒè¿›åº¦")
    else:
        print("ğŸ†• æ­£åœ¨åˆ›å»ºæ–°çš„PPOæ¨¡å‹...")
        model = PPO(
            "CnnPolicy",
            train_env,
            verbose=1,
            tensorboard_log=log_dir,
            device='auto',
            **ppo_params  # ä½¿ç”¨ ** è§£åŒ…å­—å…¸ï¼Œä¼ å…¥æ‰€æœ‰ä¼˜åŒ–åçš„è¶…å‚æ•°
        )
        print("âœ… æ–°PPOæ¨¡å‹åˆ›å»ºå®Œæˆ")
        print("--- ä½¿ç”¨çš„è¶…å‚æ•° ---")
        for key, value in ppo_params.items():
            # å¯¹ learning_rate åšç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå‡½æ•°
            if key == 'learning_rate':
                print(f"{key}: linear_schedule({learning_rate})")
            else:
                print(f"{key}: {value}")
        print("--------------------")

    print("\n" + "=" * 60)
    print("ğŸƒâ€â™‚ï¸ å¼€å§‹è®­ç»ƒ...")
    print("=" * 60)

    # å…³é”®ä¼˜åŒ–ï¼šå¢åŠ æ€»è®­ç»ƒæ­¥æ•°ã€‚Atariæ¸¸æˆéœ€è¦å¤§é‡æ ·æœ¬æ¥å­¦ä¹ 
    # 100ä¸‡æ­¥å¯¹äºAtariæ¥è¯´ä»…ä»…æ˜¯å¼€å§‹ï¼Œé€šå¸¸éœ€è¦1000ä¸‡æ­¥æˆ–æ›´å¤š
    TOTAL_TIMESTEPS = 3_000_000

    model.learn(
        total_timesteps=TOTAL_TIMESTEPS,
        tb_log_name="PPO_with_vis",
        callback=callbacks
    )

    print_training_footer(log_dir)

    # --- 5. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.save(model_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

    # --- 6. æ¸…ç†ç¯å¢ƒ ---
    print("ğŸ§¹ æ­£åœ¨æ¸…ç†ç¯å¢ƒ...")
    train_env.close()
    eval_env.close()
    print("âœ… ç¯å¢ƒæ¸…ç†å®Œæˆ")
    print("ğŸ‘‹ ç¨‹åºç»“æŸ")


if __name__ == "__main__":
    main()