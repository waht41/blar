import ale_py # ale_py éœ€è¦è¢« import ä¸€æ¬¡ï¼Œè®© gym èƒ½å¤Ÿå‘ç° Atari ç¯å¢ƒ
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack, SubprocVecEnv
from stable_baselines3.common.callbacks import CallbackList
from stable_baselines3.common.monitor import Monitor
import os
import yaml
from datetime import datetime
from typing import Callable  # å¯¼å…¥ Callable ç”¨äºå®šä¹‰å­¦ä¹ ç‡è°ƒåº¦
import gymnasium as gym
from stable_baselines3.common.atari_wrappers import (
    ClipRewardEnv,
    EpisodicLifeEnv,
    FireResetEnv,
    MaxAndSkipEnv,
    NoopResetEnv,
    WarpFrame,
)

from src.callbacks import VisualizationCallback, PerformanceCallbackWithTqdm
from src.utils.training_utils import setup_training_args_and_logs, print_training_header, print_training_footer
ale_py # å¼•å…¥ç¯å¢ƒï¼Œç”¨æ¥è®©importä¸è¢«ideè‡ªåŠ¨åˆ é™¤

def linear_schedule(initial_value: float) -> Callable[[float], float]:
    """
    åˆ›å»ºä¸€ä¸ªçº¿æ€§å­¦ä¹ ç‡è¡°å‡çš„è°ƒåº¦å™¨ã€‚
    :param initial_value: åˆå§‹å­¦ä¹ ç‡
    :return: ä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥è¿›åº¦(0-1)ï¼Œè¾“å‡ºå½“å‰å­¦ä¹ ç‡
    """

    def func(progress_remaining: float) -> float:
        """
        éšç€è®­ç»ƒçš„è¿›è¡Œï¼Œå­¦ä¹ ç‡ä» initial_value çº¿æ€§é™ä½åˆ° 0.
        progress_remaining ä» 1 çº¿æ€§é™ä½åˆ° 0.
        """
        return progress_remaining * initial_value

    return func

def make_single_atari_env(env_id: str, seed: int, noop_max: int = 30, skip: int = 4, frame_size: int = 84, **kwargs) -> gym.Env:
    """
    ä¸º Atari åˆ›å»ºä¸€ä¸ªç»è¿‡æ ‡å‡†é¢„å¤„ç†å°è£…çš„å•ä¸ªç¯å¢ƒã€‚
    è¿™ä¸ªç‰ˆæœ¬åŒ…å«äº† Monitor wrapper ç”¨äºæ—¥å¿—è®°å½•ã€‚
    :param env_id: ç¯å¢ƒID
    :param seed: éšæœºç§å­
    :param noop_max: NoopResetEnvçš„æœ€å¤§éšæœºæ“ä½œæ•°
    :param skip: MaxAndSkipEnvçš„è·³å¸§æ•°
    :param frame_size: WarpFrameçš„å¸§å¤§å°
    :param kwargs: å…¶ä»–ç¯å¢ƒå‚æ•°
    """
    env = gym.make(env_id, **kwargs)
    env.action_space.seed(seed)

    # å…³é”®ï¼šåœ¨åº”ç”¨å…¶ä»– Wrapper ä¹‹å‰æˆ–ä¹‹åï¼ˆé€šå¸¸æ˜¯è¾ƒæ—©ï¼‰æ·»åŠ  Monitor
    # Monitor éœ€è¦åœ¨ EpisodicLifeEnv ä¹‹å‰ï¼Œä»¥æ­£ç¡®è®°å½•æ¯ä¸ª"ç”Ÿå‘½"çš„ä¿¡æ¯
    env = Monitor(env)

    env = NoopResetEnv(env, noop_max=noop_max)
    env = MaxAndSkipEnv(env, skip=skip)
    env = EpisodicLifeEnv(env)
    if "FIRE" in env.unwrapped.get_action_meanings():
        env = FireResetEnv(env)
    env = WarpFrame(env, width=frame_size, height=frame_size)
    env = ClipRewardEnv(env)

    env.reset(seed=seed)
    return env

def load_config(config_path: str = "src/atari/config.yaml") -> dict:
    """
    åŠ è½½YAMLé…ç½®æ–‡ä»¶
    :param config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    :return: é…ç½®å­—å…¸
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as file:
            config = yaml.safe_load(file)
        print(f"âœ… é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_path}")
        return config
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯ï¼šé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        raise
    except yaml.YAMLError as e:
        print(f"âŒ é”™è¯¯ï¼šYAMLæ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        raise

def log_config_to_file(config: dict, log_dir: str):
    """
    å°†é…ç½®ä¿¡æ¯å†™å…¥æ—¥å¿—æ–‡ä»¶
    :param config: é…ç½®å­—å…¸
    :param log_dir: æ—¥å¿—ç›®å½•
    """
    config_log_path = os.path.join(log_dir, "config.yaml")
    try:
        with open(config_log_path, 'w', encoding='utf-8') as file:
            yaml.dump(config, file, default_flow_style=False, allow_unicode=True, indent=2)
        print(f"âœ… é…ç½®ä¿¡æ¯å·²ä¿å­˜åˆ°æ—¥å¿—: {config_log_path}")
    except Exception as e:
        print(f"âš ï¸ è­¦å‘Šï¼šæ— æ³•ä¿å­˜é…ç½®åˆ°æ—¥å¿—æ–‡ä»¶: {e}")

def main():
    """ä¸»å‡½æ•°ï¼šè®­ç»ƒAtari Breakout PPOæ¨¡å‹"""
    # --- 0. åŠ è½½é…ç½®æ–‡ä»¶ ---
    config = load_config()
    
    # --- 1. è§£æå‘½ä»¤è¡Œå‚æ•°å’Œè®¾ç½®æ—¥å¿—ç›®å½• ---
    args, log_dir, model_path = setup_training_args_and_logs(
        game_name=config['environment']['game_name'],
        model_name="atari_breakout_model"
    )
    
    # åˆ›å»ºåŸºäºtb_log_nameçš„æ¨¡å‹ä¿å­˜ç›®å½•
    model_save_config = config.get('model_save', {})

    # åœ¨log_dirä¸‹åˆ›å»ºtb_log_nameå­ç›®å½•
    model_save_dir = os.path.join(log_dir)
    os.makedirs(model_save_dir, exist_ok=True)
    
    # ç”Ÿæˆæ¨¡å‹æ–‡ä»¶å
    model_prefix = model_save_config.get('model_prefix', 'atari_breakout_model')
    if model_save_config.get('add_timestamp', True):
        timestamp_format = model_save_config.get('timestamp_format', '%Y%m%d_%H%M%S')
        timestamp = datetime.now().strftime(timestamp_format)
        model_filename = f"{model_prefix}_{timestamp}.zip"
    else:
        model_filename = f"{model_prefix}.zip"
    
    model_path = os.path.join(model_save_dir, model_filename)
    
    print(f"ğŸ“ æ¨¡å‹å°†ä¿å­˜åˆ°: {model_path}")
    
    # å°†é…ç½®ä¿¡æ¯å†™å…¥æ—¥å¿—
    if config.get('logging', {}).get('log_config', True):
        log_config_to_file(config, log_dir)

    # gym.register_envs(ale_py) # è¿™è¡Œæ˜¯ä¸éœ€è¦çš„ï¼Œåªè¦ ale_py è¢«å¯¼å…¥ï¼Œgym å°±ä¼šè‡ªåŠ¨æ³¨å†Œç¯å¢ƒ

    print_training_header("Atari Breakout")

    # --- 2. åˆ›å»ºç¯å¢ƒ ---
    print("ğŸ“¦ æ­£åœ¨åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    # ä½¿ç”¨ make_atari_env ä¼šè‡ªåŠ¨åº”ç”¨ä¸€ç³»åˆ—å…³é”®çš„ Wrapperï¼Œä¾‹å¦‚å¸§è·³è¿‡(Frame Skipping)
    env_config = config['environment']
    wrapper_config = config['wrappers']
    
    n_envs = env_config['n_envs']
    env_id = env_config['env_id']
    eval_n_envs = env_config['eval_n_envs']
    seed = env_config['seed']
    
    env_fns = [lambda i=i: make_single_atari_env(env_id, seed=i, 
                                               noop_max=wrapper_config['noop_max'],
                                               skip=wrapper_config['skip'],
                                               frame_size=wrapper_config['frame_size']) 
              for i in range(n_envs)]
    train_env = SubprocVecEnv(env_fns)
    # VecFrameStack å°†è¿ç»­çš„4å¸§å›¾åƒå †å èµ·æ¥ï¼Œè®©æ™ºèƒ½ä½“èƒ½æ„ŸçŸ¥åˆ°è¿åŠ¨æ–¹å‘
    train_env = VecFrameStack(train_env, n_stack=4)
    print(f"âœ… è®­ç»ƒç¯å¢ƒåˆ›å»ºå®Œæˆï¼Œä½¿ç”¨ {n_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ")

    eval_env = make_atari_env(env_id, n_envs=eval_n_envs, seed=seed)
    eval_env = VecFrameStack(eval_env, n_stack=4)
    print("âœ… å¯è§†åŒ–ç¯å¢ƒåˆ›å»ºå®Œæˆ")

    # --- 3. å®ä¾‹åŒ–å›è°ƒå‡½æ•° ---
    print("ğŸ¯ è®¾ç½®å›è°ƒå‡½æ•°...")
    callback_config = config['callbacks']
    training_config = config['training']
    
    callbacks_list = []
    
    # å¯è§†åŒ–å›è°ƒ
    if callback_config['visualization']['enabled']:
        vis_callback = VisualizationCallback(
            eval_env, 
            eval_freq=callback_config['visualization']['eval_freq']
        )
        callbacks_list.append(vis_callback)
        print("âœ… å¯è§†åŒ–å›è°ƒå·²å¯ç”¨")
    
    # æ€§èƒ½ç›‘æ§å›è°ƒ
    if callback_config['performance']['enabled']:
        performance_callback = PerformanceCallbackWithTqdm(
            verbose=callback_config['performance']['verbose']
        )
        callbacks_list.append(performance_callback)
        print("âœ… æ€§èƒ½ç›‘æ§å›è°ƒå·²å¯ç”¨")
    
    # ç»„åˆå¤šä¸ªå›è°ƒå‡½æ•°
    if callbacks_list:
        callbacks = CallbackList(callbacks_list)
        print(f"âœ… å›è°ƒå‡½æ•°è®¾ç½®å®Œæˆï¼Œå…±å¯ç”¨ {len(callbacks_list)} ä¸ªå›è°ƒ")
    else:
        callbacks = None
        print("âš ï¸ è­¦å‘Šï¼šæœªå¯ç”¨ä»»ä½•å›è°ƒå‡½æ•°")

    # --- 4. å®šä¹‰è¶…å‚æ•°å¹¶åˆ›å»ºæˆ–åŠ è½½ PPO æ¨¡å‹ ---

    # ä»é…ç½®æ–‡ä»¶è¯»å–PPOè¶…å‚æ•°
    ppo_config = config['ppo']
    model_config = config['model']
    
    learning_rate = ppo_config['learning_rate']
    ppo_params = {
        'n_steps': ppo_config['n_steps'],  # å¢åŠ æ¯æ¬¡æ›´æ–°æ”¶é›†çš„æ ·æœ¬æ•°ï¼Œä»¥è·å¾—æ›´ç¨³å®šçš„æ¢¯åº¦ä¼°è®¡
        'batch_size': ppo_config['batch_size'],  # å¢åŠ  mini-batch çš„å¤§å°
        'n_epochs': ppo_config['n_epochs'],  # å‡å°‘ epoch æ•°é‡ï¼Œé˜²æ­¢åœ¨å½“å‰æ•°æ®ä¸Šè¿‡æ‹Ÿåˆ
        'gamma': ppo_config['gamma'],  # æŠ˜æ‰£å› å­
        'gae_lambda': ppo_config['gae_lambda'],  # GAE-Lambda å‚æ•°
        'clip_range': ppo_config['clip_range'],  # å‡å° clip_rangeï¼Œé™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œæå‡ç¨³å®šæ€§
        'ent_coef': ppo_config['ent_coef'],  # ç†µç³»æ•°ï¼Œé¼“åŠ±æ¢ç´¢
        'vf_coef': ppo_config['vf_coef'],  # ä»·å€¼å‡½æ•°ç³»æ•°
        'learning_rate': linear_schedule(learning_rate),  # ä½¿ç”¨çº¿æ€§è¡°å‡çš„å­¦ä¹ ç‡ï¼Œä» 2.5e-4 é™åˆ° 0
    }

    if args.resume:
        print("ğŸ”„ æ­£åœ¨åŠ è½½ç°æœ‰æ¨¡å‹...")
        if not os.path.exists(args.resume):
            print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.resume}")
            return

        model = PPO.load(
            args.resume,
            env=train_env,
            reset_num_timesteps=False,
            # åŠ è½½æ¨¡å‹æ—¶ä¹Ÿæœ€å¥½ä¼ å…¥è‡ªå®šä¹‰å‚æ•°ï¼Œä»¥é˜²é»˜è®¤å‚æ•°è¦†ç›–
            custom_objects={"learning_rate": ppo_params['learning_rate']}
        )
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {args.resume}")
        print("ğŸ“Š å°†ç»§ç»­ä¹‹å‰çš„è®­ç»ƒè¿›åº¦")
    else:
        print("ğŸ†• æ­£åœ¨åˆ›å»ºæ–°çš„PPOæ¨¡å‹...")
        model = PPO(
            model_config['policy'],
            train_env,
            verbose=training_config['verbose'],
            tensorboard_log=log_dir if model_config['tensorboard_log'] else None,
            device=model_config['device'],
            **ppo_params  # ä½¿ç”¨ ** è§£åŒ…å­—å…¸ï¼Œä¼ å…¥æ‰€æœ‰ä¼˜åŒ–åçš„è¶…å‚æ•°
        )
        print("âœ… æ–°PPOæ¨¡å‹åˆ›å»ºå®Œæˆ")
        print("--- ä½¿ç”¨çš„è¶…å‚æ•° ---")
        for key, value in ppo_params.items():
            # å¯¹ learning_rate åšç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå‡½æ•°
            if key == 'learning_rate':
                print(f"{key}: linear_schedule({learning_rate})")
            else:
                print(f"{key}: {value}")
        print("--------------------")

    print("\n" + "=" * 60)
    print("ğŸƒâ€â™‚ï¸ å¼€å§‹è®­ç»ƒ...")
    print("=" * 60)

    # ä»é…ç½®æ–‡ä»¶è¯»å–æ€»è®­ç»ƒæ­¥æ•°
    TOTAL_TIMESTEPS = training_config['total_timesteps']
    logging_config = config['logging']

    model.learn(
        total_timesteps=TOTAL_TIMESTEPS,
        tb_log_name=logging_config['tb_log_name'],
        callback=callbacks
    )

    print_training_footer(log_dir)

    # --- 5. ä¿å­˜æœ€ç»ˆæ¨¡å‹ ---
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    model.save(model_path)
    print(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜è‡³: {model_path}")

    # --- 6. æ¸…ç†ç¯å¢ƒ ---
    print("ğŸ§¹ æ­£åœ¨æ¸…ç†ç¯å¢ƒ...")
    train_env.close()
    eval_env.close()
    print("âœ… ç¯å¢ƒæ¸…ç†å®Œæˆ")
    print("ğŸ‘‹ ç¨‹åºç»“æŸ")


if __name__ == "__main__":
    main()